{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "# reload sf import while I'm working on extracting functionality to it from notebooks\n",
    "from importlib import reload\n",
    "import sf_lib; import sf_lib.sf; import sf_lib.df\n",
    "reload(sf_lib), reload(sf_lib.sf), reload(sf_lib.df)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "from sf_lib.df import (\n",
    "    make_df_classify, \n",
    "    make_df_tasks_with_props\n",
    ")\n",
    "from sf_lib.sf import (\n",
    "    get_running_vote_fraction,\n",
    "    getFilename, \n",
    "    getMetadataValue, \n",
    "    parseTime, \n",
    "    getGroupSize, \n",
    "    extractTaskValue\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    df.to_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load auxiliary data, like object info and names of targets in the catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_info = pd.read_csv('../../catalogue/sf_spacefluff_object_data.csv', comment=\"#\")\n",
    "candidate_names_classify = np.loadtxt('../sf_candidate_names__classification-classify.txt', dtype='str')\n",
    "catalogue_targets = np.loadtxt('../../catalogue/sf_catalogue_targets.txt', dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tasks so we can load the dataframes properly\n",
    "\n",
    "<span style=\"color: red;\"><strong>IMPORTANT!</strong></span> When we go to combine the three dataframes, make sure to swap the names of the T1 and T2 columns in the `hardcore` workflow. For some reason, The question assigned to T1 in `Classify!` is assigned to T2 in `Hardcore`. If we don't swap them, we'll end up with useless data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_hardcore = [0, 2, 1, 3, 4, 5, 9]\n",
    "tasks_classify = [0, 1]\n",
    "tasks_onthego = [0]\n",
    "\n",
    "task_strings = ['T{}'.format(t) for t in tasks_hardcore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classify = make_df_classify('classify', tasks_classify)\n",
    "df_hardcore = make_df_classify('hardcore', tasks_hardcore)\n",
    "df_onthego = make_df_classify('onthego', tasks_onthego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swap T1 and T2 columns in `hardcore` workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hardcore[['T1', 'T2']] = df_hardcore[['T2', 'T1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack the three dataframes\n",
    "\n",
    "Note the following inconsistency: `onthego` formulates one of the _task 0_ answers as \"Group of objects (cluster)\", while `classify` and `hardcore` have it formulated with uppercase 'C': \"Group of objects (Cluster)\". Fix this by just coercing all answers to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_classify.append(df_hardcore).append(df_onthego)\n",
    "df['T0'] = df['T0'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find duplicate classifications of an object made by the same user across workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of readability, I won't be coding the most efficient way time-complexity wise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice dataframe columns we need for much faster indexing\n",
    "groupby_user = df[['user_name', 'Filename', 'T0']].groupby('user_name')\n",
    "unique_users = df['user_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through groups to find users who made more classifications than they saw unique objects,\n",
    "# i.e. they saw at least one object multiple times\n",
    "\n",
    "users_with_duplicates = []\n",
    "\n",
    "for user in unique_users:\n",
    "    clas = groupby_user.get_group(user)\n",
    "    objects_seen = clas['Filename']\n",
    "    if not objects_seen.shape[0] == objects_seen.unique().shape[0]:\n",
    "        users_with_duplicates.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list with each user and the objects they saw multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_classifications = {user: [] for user in users_with_duplicates}\n",
    "\n",
    "for user in users_with_duplicates:\n",
    "    clas = groupby_user.get_group(user)\n",
    "    objects = clas['Filename']\n",
    "    \n",
    "    objects_seen = []\n",
    "    for obj in objects:\n",
    "        if not obj in objects_seen:\n",
    "            objects_seen.append(obj)\n",
    "        else:\n",
    "            duplicate_classifications[user].append(obj)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all duplicate classifications and extract classification_id of every classification where the user had already seen that object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_filter = []  # ids will be appeneded to this list\n",
    "\n",
    "for user, dupes in duplicate_classifications.items():\n",
    "    seen = []\n",
    "\n",
    "    # query df by username and filename to get these objects\n",
    "    vals = df.query(\"user_name == @user & Filename.isin(@dupes)\")[['Filename', 'classification_id']].values\n",
    "    for entry in vals:\n",
    "        name, _id = entry\n",
    "        if name in seen:\n",
    "            to_filter.append(_id)\n",
    "        else:\n",
    "            seen.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter all classifications where user had already seen that object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"~classification_id.isin(@to_filter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite creation of 'df_tasks_with_props' for stacked dataframe\n",
    "beats wasting hours messing with df_tasks_with_props code from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_strings_hc = ['T{}'.format(t) for t in tasks_hardcore]\n",
    "task_strings_classify = ['T0', 'T1']\n",
    "\n",
    "def df_votes_with_vote_count(df_votes):\n",
    "    df_votes.insert(1, 'vote_count', df_votes['T0'].apply(lambda x: sum(list(x.values()))))\n",
    "    return df_votes\n",
    "\n",
    "def df_votes_with_object_info(df_votes):\n",
    "    df_votes = df_votes.merge(object_info, how='outer', on='name')\n",
    "    df_votes = df_votes.query(\"~vote_count.isnull()\")\n",
    "    return df_votes\n",
    "\n",
    "def get_answer_vote_percentage(row, entry, decimal_places=1):\n",
    "    none_count = row.get('None', 0)\n",
    "    total_votes = sum(row.values())\n",
    "    actual_votes = total_votes - none_count\n",
    "    \n",
    "    if actual_votes > 0:\n",
    "        return round(100*row.get(entry, 0)/actual_votes, decimal_places)\n",
    "\n",
    "tasks_hardcore = [0, 2, 1, 3, 4, 5, 9]\n",
    "task_strings = ['T{}'.format(t) for t in tasks_hardcore]\n",
    "\n",
    "def df_votes_with_vote_percentages(df_votes, df):\n",
    "    for task in task_strings:\n",
    "        for entry in df.query(\"~{}.isnull()\".format(task))[task].unique().tolist():\n",
    "            df_votes[\"{} % {}\".format(task, entry.lower())] = df_votes[task].apply(lambda x: get_answer_vote_percentage(x,  entry))\n",
    "            \n",
    "    return df_votes\n",
    "\n",
    "def make_df_votes(df, task_strings):\n",
    "    t0_answers = df['T0'].unique().tolist()\n",
    "    gr = df[['Filename', *task_strings]].groupby('Filename')\n",
    "    \n",
    "    vals_list = []\n",
    "    for name in df['Filename'].unique().tolist():\n",
    "        vals = { \"name\": name }\n",
    "        for task in tasks_hardcore:\n",
    "            t = 'T{}'.format(task)\n",
    "            vals[t] = gr.get_group(name)[t].value_counts().to_dict()\n",
    "\n",
    "        vals_list.append(vals)\n",
    "\n",
    "    df_votes = pd.DataFrame(vals_list)\n",
    "    df_votes = df_votes_with_vote_count(df_votes)\n",
    "    df_votes = df_votes_with_object_info(df_votes)\n",
    "    df_votes = df_votes_with_vote_percentages(df_votes, df)\n",
    "    \n",
    "    return df_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export `df_votes` to json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes = make_df_votes(df, task_strings_hc)\n",
    "df_to_json(df_votes, 'df_votes.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export `df` (stacked df) to json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df_to_json(df, 'df_stacked.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some statistics from the filtering process above, like # of users that saw any object multiple times, number of classifications to be filtered out, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users that saw at least one object multiple times: 233\n",
      "[# of objects seen multiple times per user,  frequency] \n",
      " [[   1  121]\n",
      " [   2   32]\n",
      " [   3   14]\n",
      " [   4    8]\n",
      " [   5    4]\n",
      " [   6    7]\n",
      " [   7    3]\n",
      " [   8    1]\n",
      " [   9    1]\n",
      " [  10    3]\n",
      " [  12    2]\n",
      " [  13    1]\n",
      " [  14    1]\n",
      " [  15    1]\n",
      " [  16    2]\n",
      " [  17    1]\n",
      " [  18    2]\n",
      " [  19    1]\n",
      " [  20    2]\n",
      " [  24    1]\n",
      " [  28    2]\n",
      " [  34    1]\n",
      " [  38    1]\n",
      " [  42    1]\n",
      " [  48    1]\n",
      " [  55    1]\n",
      " [  58    1]\n",
      " [  61    1]\n",
      " [  65    1]\n",
      " [  73    1]\n",
      " [  90    1]\n",
      " [ 102    1]\n",
      " [ 104    1]\n",
      " [ 120    1]\n",
      " [ 121    1]\n",
      " [ 142    1]\n",
      " [ 171    1]\n",
      " [ 183    1]\n",
      " [ 224    1]\n",
      " [ 231    1]\n",
      " [ 473    1]\n",
      " [ 793    1]\n",
      " [2046    1]\n",
      " [4363    1]]\n"
     ]
    }
   ],
   "source": [
    "print('# users that saw at least one object multiple times:', len(users_with_duplicates))  # Discovery: 233 users saw the same object multiple times across workflows.\n",
    "\n",
    "duplicate_count = [len(objects) for [user, objects] in duplicate_classifications.items()] \n",
    "\n",
    "# Print the frequency of duplicate votes (first entry is # of duplicates seen by user, second is the amount of users that saw that many duplicate objects)\n",
    "duplicate_count_frequency = np.unique(duplicate_count, return_counts=True)\n",
    "print('[# of objects seen multiple times per user,  frequency]', '\\n', np.array(duplicate_count_frequency).T)  # Discovery: there is one user that saw 4363 duplicate objects, and one that saw 2046 duplicates. What happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract usernames of users that saw more than 1000 duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_users = list(filter(lambda entry: len(entry[1]) > 1000, duplicate_classifications.items()))\n",
    "strange_users = [user[0] for user in strange_users]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print number of votes cast per option for T0 (task 0) by these users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'galaxy': 3422, 'group of objects (cluster)': 1724, 'something else/empty center': 805}\n",
      "# votes by user: 5951\n",
      "\n",
      "\n",
      "{'galaxy': 3837, 'group of objects (cluster)': 1233, 'something else/empty center': 690}\n",
      "# votes by user: 5760\n"
     ]
    }
   ],
   "source": [
    "for user in strange_users:\n",
    "    print('\\n')\n",
    "    print(df.query(\"user_name == @user\")['T0'].value_counts().to_dict())\n",
    "    print('# votes by user:', df.query(\"user_name == @user\").shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print # of 'duplicate' classifications that were filtered out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223059"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classifications filtered as duplicates: 10316\n"
     ]
    }
   ],
   "source": [
    "print('Number of classifications filtered as duplicates:', len(to_filter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovery: there are 10316 classifications (approx. 5% of the total, actually, way  more than I expected) made by users that had already seen that object at least once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @idea: plot distribution of votes per user between galaxy, group, empty center as a function of votes per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovery (cells deleted): classification may have retired state without me having noticed it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols(df, cols):\n",
    "    return df[cols].T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_plot(x, y, \n",
    "   xlabel, ylabel, title, \n",
    "   hist1=None, hist2=None, \n",
    "   savepath=None, \n",
    "   figsize=(12,8), \n",
    "   invert_x=True, invert_y=True\n",
    "):\n",
    "    if not hist1:\n",
    "        hist1 = x\n",
    "    if not hist2:\n",
    "        hist2 = y\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = fig.add_gridspec(5,5, wspace=0, hspace=0)\n",
    "\n",
    "    fr1 = fig.add_subplot(gs[:-1,1:])\n",
    "    fr2 = fig.add_subplot(gs[:-1, 0])\n",
    "    fr3 = fig.add_subplot(gs[-1, 1:])\n",
    "\n",
    "    common_hist_args = {\n",
    "        \"bins\": 50, \n",
    "        \"color\": '#ddd',\n",
    "        'histtype': 'step',\n",
    "        \"edgecolor\": '#333'\n",
    "    }\n",
    "\n",
    "    common_args = {\n",
    "        's': 6,\n",
    "        \"alpha\": 0.6,\n",
    "        \"marker\": \"D\",\n",
    "    #     \"facecolors\": 'xkcd:lightish blue',\n",
    "        \"facecolors\": 'none',\n",
    "        \"edgecolors\": \"xkcd:lightish blue\",\n",
    "        \"lw\": 1\n",
    "    }\n",
    "\n",
    "    fr2.hist(y, orientation='horizontal', **common_hist_args)\n",
    "    fr3.hist(x, **common_hist_args)\n",
    "\n",
    "    fr1.scatter(x, y, **common_args)\n",
    "    fr1.grid(alpha=0.3, which='both')\n",
    "    \n",
    "    # fr1.set_ylabel(r'$\\mu_{e,r}$ [mag arcsec$^{-2}$]', fontsize=10)\n",
    "\n",
    "    fr1.set_xlabel(xlabel, bbox={'alpha': 0.75, 'color': 'white'}, fontsize=12)\n",
    "    fr1.set_ylabel(ylabel, bbox={'alpha': 0.75, 'color': 'white'}, fontsize=12)\n",
    "\n",
    "    tick_params = {\n",
    "        \"pad\": -15,\n",
    "        \"left\": \"off\",\n",
    "        \"labelleft\": \"on\"\n",
    "    }\n",
    "    \n",
    "    fr1.tick_params(axis=\"y\", pad=-25, left='off', labelleft='off')\n",
    "    fr1.tick_params(axis=\"x\", **tick_params)\n",
    "\n",
    "    if invert_x:\n",
    "        fr1.set_xlim(fr1.get_xlim()[::-1])\n",
    "        fr3.set_xlim(fr3.get_xlim()[::-1])\n",
    "        \n",
    "    if invert_y:\n",
    "        fr1.set_ylim(fr1.get_ylim()[::-1])\n",
    "        fr2.set_ylim(fr2.get_ylim()[::-1])\n",
    "        \n",
    "    fr2.set_xlim(fr2.get_xlim()[::-1])\n",
    "    fr3.set_ylim(fr3.get_ylim()[::-1])\n",
    "\n",
    "    fr2.set_axis_off()\n",
    "    fr3.set_axis_off()\n",
    "\n",
    "#     fr1.set_zorder(100)\n",
    "    # plt.setp(fr1.get_xticklabels(), bbox={'alpha': 0.7,'color': 'white'})\n",
    "    # plt.setp(fr1.get_yticklabels(), bbox={'alpha': 0.7,'color': 'white'})\n",
    "\n",
    "    fr1.set_title(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=400)\n",
    "        \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
