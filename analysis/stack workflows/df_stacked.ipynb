{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "The goal of this notebook is to extract data from the raw .csv files from each workflow, and combine them into two final dataframes, which we then parse to .json files again to save time later on (it's much faster to import a .json file than to re-generate the dataframes every time)\n",
    "\n",
    "The two dataframes for which we'll create .json files:\n",
    "- `df`: Dataframe where each row contains a single classification. \n",
    "- `df_votes`: Derived dataframe where each row corresponds to a single object, its (photometric) parameters derived by Venhola et al. using GALFIT, and the vote distributions for that object for every task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you run this notebook:\n",
    "The following dataframes will be created as .json files:\n",
    "- `df_stacked.json`\n",
    "    - Each row contains an individual classification. Contains all non-duplicate classifications from all workflows.\n",
    "- `df_votes.json`\n",
    "    - Each row contains a single object, its properties, and its vote distributions (counts per task, and percentage per answer per task)\n",
    "- `df_stacked_exclude_n.json`, for $n \\in \\{5, 10, 25, 50\\}$\n",
    "    - Same as df_votes.json, except here we exclude the first $n$ votes per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# need to append the directory where sf_lib is stored to system path \n",
    "#  this is one way to enable us to import sf_lib, where we store functions we use in multiple notebooks\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sf_lib.df import (\n",
    "    make_df_classify, \n",
    ")\n",
    "\n",
    "from importlib import reload\n",
    "import sf_lib.helpers\n",
    "reload(sf_lib.helpers)\n",
    "from sf_lib.helpers import (\n",
    "    df_to_json,\n",
    "    fleiss_kappa\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load auxiliary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe with (GALFIT) object parameters (extracted from the likely ground truth catalogue dataset)\n",
    "#  this .csv was created in one of the notebooks in the /analysis/catalogue directory of this project\n",
    "object_info = pd.read_csv('../../catalogue/sf_spacefluff_object_data.csv', comment=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>RA</th>\n",
       "      <th>DEC</th>\n",
       "      <th>Reff</th>\n",
       "      <th>r_mag</th>\n",
       "      <th>g_mag</th>\n",
       "      <th>axis_ratio</th>\n",
       "      <th>pos_angle</th>\n",
       "      <th>n</th>\n",
       "      <th>u</th>\n",
       "      <th>...</th>\n",
       "      <th>ge</th>\n",
       "      <th>re</th>\n",
       "      <th>ie</th>\n",
       "      <th>Reffe</th>\n",
       "      <th>r_mage</th>\n",
       "      <th>ne</th>\n",
       "      <th>C</th>\n",
       "      <th>mue_r</th>\n",
       "      <th>bae</th>\n",
       "      <th>RFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UDGcand_0</td>\n",
       "      <td>56.232609</td>\n",
       "      <td>-35.335724</td>\n",
       "      <td>2.8934</td>\n",
       "      <td>19.8319</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.6034</td>\n",
       "      <td>-27.775499</td>\n",
       "      <td>1.0271</td>\n",
       "      <td>22.426844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054816</td>\n",
       "      <td>0.046306</td>\n",
       "      <td>0.044244</td>\n",
       "      <td>0.332842</td>\n",
       "      <td>0.131324</td>\n",
       "      <td>0.117142</td>\n",
       "      <td>2.76421</td>\n",
       "      <td>23.585331</td>\n",
       "      <td>0.027318</td>\n",
       "      <td>-99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        name         RA        DEC    Reff    r_mag  g_mag  axis_ratio  \\\n",
       "0  UDGcand_0  56.232609 -35.335724  2.8934  19.8319   -1.0      0.6034   \n",
       "\n",
       "   pos_angle       n          u  ...        ge        re        ie     Reffe  \\\n",
       "0 -27.775499  1.0271  22.426844  ...  0.054816  0.046306  0.044244  0.332842   \n",
       "\n",
       "     r_mage        ne        C      mue_r       bae   RFF  \n",
       "0  0.131324  0.117142  2.76421  23.585331  0.027318 -99.0  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to inspect the first row of the object_info dataframe for reference purposes\n",
    "object_info.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tasks so we can load the dataframes properly\n",
    "\n",
    "The three workflows differ in which questions ('tasks') they present to users.\n",
    "\n",
    "<span style=\"color: red;\"><strong>IMPORTANT!</strong></span> When we go to combine the three dataframes, we need to swap the names of the T1 and T2 columns in the `hardcore` workflow. For some reason, The question assigned to T1 in `Classify!` is assigned to T2 in `Hardcore`. If we don't swap them, we'll end up with useless data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_hardcore = [0, 2, 1, 3, 4, 5, 9]\n",
    "tasks_classify = [0, 1]\n",
    "tasks_onthego = [0]\n",
    "\n",
    "# hardcore contains all tasks, so use this to generate the final dataframe.\n",
    "#  classifications that are from other workflows will simply have NaN or None \n",
    "#  as values for tasks that aren't present in that workflow\n",
    "task_strings = ['T{}'.format(t) for t in tasks_hardcore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classify = make_df_classify('classify', tasks_classify)\n",
    "df_hardcore = make_df_classify('hardcore', tasks_hardcore)\n",
    "df_onthego = make_df_classify('onthego', tasks_onthego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swap T1 and T2 columns in `hardcore` workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hardcore[['T1', 'T2']] = df_hardcore[['T2', 'T1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack the three dataframes\n",
    "\n",
    "Note the following inconsistency: `onthego` formulates one of the _task 0_ answers as \"Group of objects (cluster)\", while `classify` and `hardcore` have it formulated with uppercase _C_: \"Group of objects (Cluster)\". \n",
    "\n",
    "__Fix this by just coercing all answers to lowercase:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_classify.append(df_hardcore).append(df_onthego)\n",
    "df['T0'] = df['T0'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sort classifications by date__. This makes it easier to filter classifications made by the same user, of the same object, in multiple workflows (we will only keep the first classification of an object in cases where a user saw it in multiple workflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classifications across all three workflows: 233375\n"
     ]
    }
   ],
   "source": [
    "print('Total number of classifications across all three workflows:', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find duplicate classifications of an object made by the same user across workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'duplicate' classification is one where a single user saw a single object (e.g. `UDGcand_001`) in multiple workflows (e.g. `Hardcore` and `Classify!`). The duplicates are the classifications after the one in the first workflow where the user saw that object.\n",
    "\n",
    "For code clarity purposes, I won't be coding the most efficient way time-complexity wise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice dataframe columns we need for much faster indexing\n",
    "groupby_user = df[['user_name', 'Filename', 'T0']].groupby('user_name')\n",
    "unique_users = df['user_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through groups to find users who made more classifications than they saw unique objects,\n",
    "#  i.e. they saw at least one object multiple times\n",
    "users_with_duplicates = []\n",
    "\n",
    "for user in unique_users:\n",
    "    classifications_by_user = groupby_user.get_group(user)\n",
    "    objects_seen_by_user = classifications_by_user['Filename']\n",
    "    \n",
    "    if not objects_seen_by_user.shape[0] == objects_seen_by_user.unique().shape[0]:\n",
    "        users_with_duplicates.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users with 'duplicate' classifications: 233\n"
     ]
    }
   ],
   "source": [
    "print('Total number of users with \\'duplicate\\' classifications:', len(users_with_duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list with each user and the objects they saw multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_classifications = {user: [] for user in users_with_duplicates}\n",
    "\n",
    "for user in users_with_duplicates:\n",
    "    classifications_by_user = groupby_user.get_group(user)\n",
    "    objects = classifications_by_user['Filename']\n",
    "    \n",
    "    objects_seen_by_user = []\n",
    "    for obj in objects:\n",
    "        if not obj in objects_seen_by_user:\n",
    "            objects_seen_by_user.append(obj)\n",
    "        else:\n",
    "            duplicate_classifications[user].append(obj)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all duplicate classifications and extract classification_id of every classification where the user had already seen that object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_filter = []  # this will become a list of `classification_id`\n",
    "\n",
    "for user, dupes in duplicate_classifications.items():\n",
    "    objects_seen = []\n",
    "\n",
    "    # query df by username and filename to get classification_ids of 'duplicates'\n",
    "    vals = df.query(\"user_name == @user & Filename.isin(@dupes)\")[['Filename', 'classification_id']].values\n",
    "    for entry in vals:\n",
    "        name, clas_id = entry\n",
    "        if name in objects_seen:\n",
    "            to_filter.append(clas_id)\n",
    "        else:\n",
    "            objects_seen.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter all classifications where that user had already seen that object (in another workflow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"~classification_id.isin(@to_filter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classifications across all three workflows after filtering duplicates: 223059\n"
     ]
    }
   ],
   "source": [
    "print('Total number of classifications across all three workflows after filtering duplicates:', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `df_votes` dataframe as mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_votes_with_vote_count(df_votes):\n",
    "    'Append column containing the total number of votes an object received from users'\n",
    "    df_votes.insert(1, 'vote_count', df_votes['T0'].apply(lambda x: sum(list(x.values()))))\n",
    "    return df_votes\n",
    "\n",
    "def df_votes_with_object_info(df_votes):\n",
    "    'Assign (photometric) properties to each object by merging `df_votes` dataframe with `object_info` dataframe'\n",
    "    df_votes = df_votes.merge(object_info, how='outer', on='name')\n",
    "    df_votes = df_votes.query(\"~vote_count.isnull()\")\n",
    "    return df_votes\n",
    "\n",
    "def get_answer_vote_percentage(row, unique_answer, decimal_places=1):\n",
    "    '''\n",
    "        Assign the percentage of votes given for the specified unique answer\n",
    "        @param row: the task column (e.g. `T0`) of a single row from a `df_votes`-type dataframe\n",
    "            where a task column looks like (using T0 as example) \n",
    "                {\n",
    "                    'galaxy': 10,\n",
    "                    'group of objects (cluster)': 2,\n",
    "                    'something else/empty center': 0\n",
    "                }\n",
    "        @param unique_answer: one of the unique answers provided for a task (e.g. for task 0, one of the unique answers is `galaxy`)\n",
    "            in the above T0 example, unique_answer may be either 'galaxy', 'group of objects (cluster)' or 'something else/empty center',\n",
    "            but it could also be None\n",
    "        @returns percentage of votes for the given answer, rounded to 1 decimal place by default\n",
    "    '''\n",
    "    \n",
    "    none_count = row.get('None', 0)\n",
    "    total_votes = sum(row.values())\n",
    "    actual_votes = total_votes - none_count\n",
    "    \n",
    "    if actual_votes > 0:\n",
    "        return round(100*row.get(unique_answer, 0)/actual_votes, decimal_places)\n",
    "\n",
    "def df_votes_with_vote_percentages(df_votes, df):\n",
    "    '''\n",
    "        Assign vote percentages for every unique answer from each unique task to every object using get_answer_vote_percentage\n",
    "        @param df_votes: `df_votes`-like dataframe (where every row contains one object, all its votes for each task, and its (photometric) properties)\n",
    "        @param df: `df`-like dataframe (where every row contains one classification)\n",
    "        @returns `df_votes` with new a new column containing the percentage of votes it received for every unique answer for each task\n",
    "    '''\n",
    "    for task in task_strings:\n",
    "        \n",
    "        for unique_answer in df.query(\"~{}.isnull()\".format(task))[task].unique().tolist():\n",
    "            if not task == 'T0':\n",
    "                unique_answer = unique_answer.capitalize()\n",
    "            df_votes[\"{} % {}\".format(task, unique_answer.lower())] = df_votes[task].apply(lambda x: get_answer_vote_percentage(x,  unique_answer))\n",
    "            \n",
    "    return df_votes\n",
    "\n",
    "def make_df_votes(df, task_strings):\n",
    "    '''\n",
    "        Convert a `df`-like dataframe into a `df_votes` dataframe, \n",
    "            where each row contains a single object and all its votes and (photometric) properties\n",
    "        @param df: `df`-like dataframe (where each row contains one classification) \n",
    "        @param task_strings: list of all tasks for which to extract vote counts and percentages, e.g. ['T0', 'T1']\n",
    "    '''\n",
    "    groupby_name = df[['Filename', *task_strings]].groupby('Filename')\n",
    "    \n",
    "    vals_list = []\n",
    "    for object_name in df['Filename'].unique().tolist():\n",
    "        vals = { \"name\": object_name }\n",
    "        for task in tasks_hardcore:\n",
    "            t = 'T{}'.format(task)\n",
    "            vals[t] = groupby_name.get_group(object_name)[t].value_counts().to_dict()\n",
    "\n",
    "        vals_list.append(vals)\n",
    "\n",
    "    df_votes = pd.DataFrame(vals_list)\n",
    "    df_votes = df_votes_with_vote_count(df_votes)\n",
    "    df_votes = df_votes_with_object_info(df_votes)\n",
    "    df_votes = df_votes_with_vote_percentages(df_votes, df)\n",
    "    \n",
    "    return df_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export `df_votes` to .json (each row contains all votes and properties of a single object) to json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcore contains all tasks, so use this to generate the final dataframe.\n",
    "#  classifications that are from other workflows will simply have NaN or None \n",
    "#  as values for tasks that aren't present in that workflow\n",
    "task_strings_hc = ['T{}'.format(t) for t in tasks_hardcore]\n",
    "\n",
    "df_votes = make_df_votes(df, task_strings_hc)\n",
    "df_to_json(df_votes, 'df_votes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export `df` to .json (each row contains a single classification. Contains all non-duplicate classifications from all workflows) to json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df_to_json(df, 'df_stacked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics from the filtering process above, like # of users that saw any object multiple times, number of classifications to be filtered out, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users that saw at least one object multiple times: 233\n"
     ]
    }
   ],
   "source": [
    "print('Number of users that saw at least one object multiple times:', len(users_with_duplicates))  # Discovery: 233 users saw the same object multiple times across workflows.\n",
    "\n",
    "duplicate_count = [len(objects) for [user, objects] in duplicate_classifications.items()] \n",
    "\n",
    "## uncomment to print the frequency of duplicate votes (first entry is # of duplicates seen by user, second is the amount of users that saw that many duplicate objects)\n",
    "# duplicate_count_frequency = np.unique(duplicate_count, return_counts=True)\n",
    "# print('[# of objects seen multiple times per user,  frequency]', '\\n', np.array(duplicate_count_frequency).T)  # Discovery: there is one user that saw 4363 duplicate objects, and one that saw 2046 duplicates. What happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract usernames of users that saw more than 1000 duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_many_duplicates = list(filter(lambda entry: len(entry[1]) > 1000, duplicate_classifications.items()))\n",
    "has_many_duplicates = [user[0] for user in has_many_duplicates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print number of votes cast per option for T0 (task 0) by these users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'galaxy': 3422, 'group of objects (cluster)': 1724, 'something else/empty center': 805}\n",
      "# votes by user: 5951\n",
      "\n",
      "\n",
      "{'galaxy': 3837, 'group of objects (cluster)': 1233, 'something else/empty center': 690}\n",
      "# votes by user: 5760\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for user in has_many_duplicates:\n",
    "    print(df.query(\"user_name == @user\")['T0'].value_counts().to_dict())\n",
    "    print('# votes by user:', df.query(\"user_name == @user\").shape[0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print number of duplicate classifications that were filtered out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classifications filtered as duplicates: 10316\n"
     ]
    }
   ],
   "source": [
    "print('Number of classifications filtered as duplicates:', len(to_filter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovery: Of the total 233375 classifications, there are 10316 classifications (approx. 4.4% of the total) made by users that had already seen that object at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude first $n$ classifications per user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_first_n(df, n):\n",
    "    '''\n",
    "        Filter df by excluding first `n` classifications done by each user\n",
    "        @param df: input dataframe\n",
    "        @param n (int): number of classifications per user to exclude\n",
    "        @returns filtered dataframe, total number of excluded classifications\n",
    "    '''\n",
    "    groupby_user = df[['user_name', 'classification_id', 'created_at']].groupby('user_name')\n",
    "\n",
    "    exclude_ids = []\n",
    "    for name, group in groupby_user:\n",
    "        ids_to_exclude = group['classification_id'].tolist()[:n]\n",
    "        exclude_ids.append(ids_to_exclude)\n",
    "    \n",
    "    exclude_ids = np.concatenate(exclude_ids)\n",
    "    return df.query('~classification_id.isin(@exclude_ids)'), exclude_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_votes_exclude_n(df, task_strings, n):\n",
    "    'Create and save to .json a version of df_votes with the first n classifications per user filtered out'\n",
    "    \n",
    "    df_exclude_n, num_excluded = exclude_first_n(df, n)\n",
    "    \n",
    "    df_votes_exclude_n = make_df_votes(df_exclude_n, task_strings)\n",
    "    \n",
    "    df_to_json(df_votes_exclude_n, 'df_votes_exclude_{}'.format(n))\n",
    "    print('saved df_votes_exclude_{}.json'.format(n))\n",
    "    print('Total number of classifications excluded: {}'.format(num_excluded))\n",
    "    print('\\n')\n",
    "    return df_votes_exclude_n, num_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_counts = [5, 10, 25, 50, 250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uncomment the next cell to create `df_votes` dataframes resulting from excluding the first $n$ classifications per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create df_votes_exclude_`n`.json for various `n`\n",
    "# #  running this cell takes a while, and we could speed it up by using dynamic programming, \n",
    "# #  but this is the most readable format and we should only need to run the cell once anyway\n",
    "\n",
    "# for n in exclude_counts:\n",
    "#     make_df_votes_exclude_n(df, task_strings_hc, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include _all_ votes, but only of power users (e.g. users that made at least 250 votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_users = []\n",
    "for name, group in df.groupby('user_name'):\n",
    "    if group.shape[0] >= 250:\n",
    "        power_users.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power_users = df.query('user_name.isin(@power_users)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes_power_users = make_df_votes(df_power_users, task_strings_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(df_votes_power_users, 'df_votes_power_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fleiss $\\kappa$ for task 0 for the dataframes excluding the first $n$ classifications per user for various $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_counts = [5, 10, 25, 50, 250]\n",
    "dfs_votes_excluding_n = [pd.read_json('df_votes_exclude_{}.json'.format(n)) for n in exclude_counts]\n",
    "dfs_excluding_n = [exclude_first_n(df, n)[0] for n in exclude_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclude n | Fleiss kappa\n",
      "------------------------------\n",
      "        5 | 0.6736646178382614\n",
      "       10 | 0.6783956776300358\n",
      "       25 | 0.688200672137247\n",
      "       50 | 0.7008247323197264\n",
      "      250 | 0.7765682246740122\n"
     ]
    }
   ],
   "source": [
    "# print Fleiss kappa for task 0 while excluding various numbers of leading classifications per user\n",
    "\n",
    "print('Exclude n | Fleiss kappa')\n",
    "print('-'*30)\n",
    "for i, (d, d_v) in enumerate(zip(dfs_excluding_n, dfs_votes_excluding_n)):\n",
    "    print('{:9.0f} | {}'.format(exclude_counts[i], fleiss_kappa(d, d_v, 'T0')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
