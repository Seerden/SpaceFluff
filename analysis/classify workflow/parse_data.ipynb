{
 "cells": [
  {
   "source": [
    "## Goal\n",
    "Take `classify_classifications.csv` and parse it into various dataframes for analysis. Do this in this notebook to keep the length of our analysis notebooks manageable.\n",
    "\n",
    "@note: Actually, I'd rather just create functions that create/extract dataframes, since extracting JSON columns, saving them to csv, and extracting again, becomes a bit tricky."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from dateutil.parser import parse\n",
    "from datetime import date\n",
    "\n",
    "import sys \n",
    "sys.path.append('../..')\n",
    "from sf_lib.sf import getFilename, parseTime, extract_task_value, percentageVotesForAnswer\n",
    "from sf_lib.df import make_df_classify, make_df_task0, make_df_tasks_with_props\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique object names that resulted from another notebook \n",
    "candidate_names_classify = np.loadtxt('../sf_candidate_names__classification-classify.txt', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Parse the dataframe itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_df_classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 1783, 'user_ip': 1157, 'user_id': 1136}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df[df.user_name.isnull()]  # returns empty df, so every classification has a user_name associated with it\n",
    "df[df.user_ip.isnull()]  # also returns empty df\n",
    "\n",
    "unique_entries = {\n",
    "    \"user_name\": df.user_name.unique().shape[0],\n",
    "    \"user_ip\": df.user_ip.unique().shape[0],\n",
    "    \"user_id\": df.user_id.unique().shape[0]\n",
    "}\n",
    "\n",
    "unique_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the above shows that there's more unique usernames than either ips or ids. Assume multiple people may share an IP, and note that not all classifications have an associated user_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Create 'task0' dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task0 = make_df_task0(df, candidate_names_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Create 'df_retired' and 'df_with_props' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_props = make_df_with_props(df, candidate_names_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Create 'task1' dataframe, and merge it with 'task0' dataframe to get 'tasks' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_info = pd.read_csv('../../catalogue/sf_spacefluff_object_data.csv', comment=\"#\")\n",
    "\n",
    "df_tasks_with_props = make_df_tasks_with_props(df, candidate_names_classify, object_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tasks[~df_tasks['% None'].isnull() & df_tasks['% None'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we end up with the following dataframes:\n",
    "- `df`: \n",
    "        parsed version of the complete data set\n",
    "        \n",
    "- `df_galaxy`: \n",
    "        filtered version of `df` leaving classifications where task0 == 'galaxy'\n",
    "        \n",
    "- `df_retired`, `df_props`: \n",
    "        temporary dataframes, both used to create `df_with_props`\n",
    "        \n",
    "- `df_task0`:\n",
    "        Contains the name of each galaxy, the total number of votes, and the percentage of votes for each option from task0.\n",
    "        \n",
    "- `df_with_props`:\n",
    "        version of df_task0 with the properties of each galaxy merged onto it\n",
    "        \n",
    "- `df_task1`:\n",
    "        contains the name of each galaxy, and the percentage of votes for 'fluffy' and 'bright' for task1, asked as a follow-up when people answered 'Galaxy' for task0.\n",
    "        \n",
    "- `df_tasks`:\n",
    "        df_task1 outer joined onto df_task0\n",
    "        \n",
    "- `df_tasks_with_props`:\n",
    "        df_tasks merged with object info (from Venhola's catalogue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python390jvsc74a57bd0bf9f9117a81b9c9429557cda2b87be7ef3b1b0b1cf7cef0eda783a94c339c64a",
   "display_name": "Python 3.9.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}