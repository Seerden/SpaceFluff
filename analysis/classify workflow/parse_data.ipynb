{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Take `classify_classifications.csv` and parse it into various dataframes for analysis. Do this in this notebook to keep the length of our analysis notebooks manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from dateutil.parser import parse\n",
    "from datetime import date\n",
    "\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from sf import getFilename, parseTime, extract_task_value, percentageVotesForAnswer, make_df_task0, make_df_with_props, make_df_tasks_with_props\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sf' from '..\\\\sf.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload sf import while I'm working on extracting functionality to it from notebooks\n",
    "from importlib import reload\n",
    "import sf\n",
    "reload(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique object names that resulted from another notebook \n",
    "candidate_names_classify = np.loadtxt('../sf_candidate_names__classification-classify.txt', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UDGcand_0', 'UDGcand_2', 'UDGcand_3', ..., 'UDGcand_7266',\n",
       "       'UDGcand_7268', 'UDGcand_7269'], dtype='<U12')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_names_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Parse the dataframe itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "df = pd.read_csv('../../SpaceFluff/zooniverse_exports/classify-classifications.csv', delimiter=\",\")\n",
    "\n",
    "# JSON parse the columns that were stringified\n",
    "columns_to_parse = ['annotations', 'subject_data', 'metadata']\n",
    "\n",
    "for column in columns_to_parse:\n",
    "    df[column] = df[column].apply(json.loads)\n",
    "    \n",
    "# extract filename, task0 and task1 values to new dataframe columns\n",
    "df['Filename'] = df['subject_data'].apply(getFilename)\n",
    "df['Task0'] = df['annotations'].apply(lambda x: extract_task_value(0, x))\n",
    "df['Task1'] = df['annotations'].apply(lambda x: extract_task_value(1, x))\n",
    "\n",
    "# finally, remove all rows where task0 wasn't answered (because the row, then, is useless)\n",
    "df = df[~df['Task0'].isnull()]\n",
    "\n",
    "# filter out classifications from beta\n",
    "df['created_at'] = parseTime(df['created_at'])\n",
    "end_of_beta = pd.Timestamp(date(2020,10,20), tz='utc')\n",
    "df = df[df['created_at'] > end_of_beta]\n",
    "\n",
    "# create temporary isRetired and alreadySeen rows\n",
    "df['isRetired'] = df['metadata'].apply(lambda x: x['subject_selection_state']['retired'])\n",
    "df['alreadySeen'] = df['metadata'].apply(lambda x: x['subject_selection_state']['already_seen'])\n",
    "\n",
    "# remove rows where isRetired or alreadySeen\n",
    "df = df[~df['isRetired'] & ~df['alreadySeen']]\n",
    "\n",
    "# remove isRetired and alreadySeen columns since they're obsolete hereafter\n",
    "df.drop(['isRetired', 'alreadySeen'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 1783, 'user_ip': 1157, 'user_id': 1136}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.user_name.isnull()]  # returns empty df, so every classification has a user_name associated with it\n",
    "df[df.user_ip.isnull()]  # also returns empty df\n",
    "\n",
    "unique_entries = {\n",
    "    \"user_name\": df.user_name.unique().shape[0],\n",
    "    \"user_ip\": df.user_ip.unique().shape[0],\n",
    "    \"user_id\": df.user_id.unique().shape[0]\n",
    "}\n",
    "\n",
    "unique_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the above shows that there's more unique usernames than either ips or ids. Assume multiple people may share an IP, and note that not all classifications have an associated user_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Create 'task0' dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task0 = make_df_task0(df, candidate_names_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Create 'df_retired' and 'df_with_props' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_props = make_df_with_props(df, candidate_names_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Create 'task1' dataframe, and merge it with 'task0' dataframe to get 'tasks' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_info = pd.read_csv('../../catalogue/sf_spacefluff_object_data.csv', comment=\"#\")\n",
    "\n",
    "df_tasks_with_props = make_df_tasks_with_props(df, candidate_names_classify, object_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tasks[~df_tasks['% None'].isnull() & df_tasks['% None'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we end up with the following dataframes:\n",
    "- `df`: \n",
    "        parsed version of the complete data set\n",
    "        \n",
    "- `df_galaxy`: \n",
    "        filtered version of `df` leaving classifications where task0 == 'galaxy'\n",
    "        \n",
    "- `df_retired`, `df_props`: \n",
    "        temporary dataframes, both used to create `df_with_props`\n",
    "        \n",
    "- `df_task0`:\n",
    "        Contains the name of each galaxy, the total number of votes, and the percentage of votes for each option from task0.\n",
    "        \n",
    "- `df_with_props`:\n",
    "        version of df_task0 with the properties of each galaxy merged onto it\n",
    "        \n",
    "- `df_task1`:\n",
    "        contains the name of each galaxy, and the percentage of votes for 'fluffy' and 'bright' for task1, asked as a follow-up when people answered 'Galaxy' for task0.\n",
    "        \n",
    "- `df_tasks`:\n",
    "        df_task1 outer joined onto df_task0\n",
    "        \n",
    "- `df_tasks_with_props`:\n",
    "        df_tasks merged with object info (from Venhola's catalogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Export df_tasks_with_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tasks_with_props.to_csv('./tasks_with_props.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
