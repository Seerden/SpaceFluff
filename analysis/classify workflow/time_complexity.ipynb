{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd0bf9f9117a81b9c9429557cda2b87be7ef3b1b0b1cf7cef0eda783a94c339c64a",
   "display_name": "Python 3.9.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Goal: Compare time complexity of my df creation functions to find the most efficient method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Findings:\n",
    "- Don't use parseTime as a 'converter' in the `pd.read_csv` call. Takes 8 times as long otherwise.\n",
    "- Use json_parser as converter for the columns we need to use json.loads() on. Saves ~10% of time.\n",
    "- Implementing custom value_counts().to_dict() is much faster if we can guarantee the identity of the values (which we can, in our case)\n",
    "- get_group is much faster on groups that are slices of their parent dataframe:\n",
    "    - So this:\n",
    "    ```python\n",
    "        grouped = df[[field1, field2]]).groupby(field1)\n",
    "        grouped.get_group(field1value)\n",
    "    ```\n",
    "    - can be considerably faster than this:\n",
    "    ```python\n",
    "        grouped = df.groupby(field1)\n",
    "        grouped.get_group(field1value)\n",
    "    ```\n",
    "\n",
    "\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from dateutil.parser import parse\n",
    "from datetime import date\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from sf_lib.df import make_df_classify, make_df_tasks_with_props, make_df_vote_threshold, make_df_task0\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'sf_lib.df' from '../..\\\\sf_lib\\\\df.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 499
    }
   ],
   "source": [
    "# reload sf import while I'm working on extracting functionality to it from notebooks\n",
    "from importlib import reload\n",
    "import sf_lib\n",
    "reload(sf_lib)\n",
    "reload(sf_lib.sf)\n",
    "reload(sf_lib.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_info = pd.read_csv('../../catalogue/sf_spacefluff_object_data.csv', comment=\"#\")\n",
    "candidate_names_classify = np.loadtxt('../sf_candidate_names__classification-classify.txt', dtype='str')\n",
    "catalogue_targets = np.loadtxt('../../catalogue/sf_catalogue_targets.txt', dtype='str')\n",
    "answer_types = ['Galaxy', 'Group of objects (Cluster)', 'Something else/empty center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.58 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "df = make_df_classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.34 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "make_df_tasks_with_props(df, candidate_names_classify, object_info)"
   ]
  }
 ]
}